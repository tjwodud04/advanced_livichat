# scripts/services.py
import base64
import asyncio
import threading
import json
import requests
import datetime
import random
import re
import time
from typing import Dict, Any, List, Tuple, Literal, Optional

from flask import jsonify, abort, request, Response, stream_with_context
from openai import AsyncOpenAI

from scripts.config import (
    VERCEL_TOKEN, VERCEL_PROJ_ID,
    CHARACTER_SYSTEM_PROMPTS, CHARACTER_VOICE,
    EMOTION_LINKS, HISTORY_MAX_LEN
)
from scripts.utils import (
    remove_empty_parentheses, markdown_to_html_links,
    extract_first_markdown_url, remove_emojis
)

# â–¼ í”„ë¡œì•¡í‹°ë¸Œ ì •ì±…(ì¿¨ë‹¤ìš´/ê±°ì ˆë¥ /ê°œì¸í™” ë°´ë”§) â€” ë³„ë„ ëª¨ë“ˆ
from scripts.proactive import ProactivePolicy, SuggestionType

# ======================================================================================
# ê¸€ë¡œë²Œ ìƒíƒœ
# ======================================================================================
conversation_history: List[Dict[str, Any]] = []
history_lock = threading.Lock()

# í”„ë¡œì•¡í‹°ë¸Œ ì •ì±…/ì„¸ì…˜ ìƒíƒœ
_policy = ProactivePolicy()
_last_user_utter_ts: Dict[str, float] = {}  # session_id -> last user ts

# ======================================================================================
# ë§í¬ í›„ì²˜ë¦¬ ìœ í‹¸
# ======================================================================================
URL_RE    = re.compile(r'(https?://[^\s<>"\']+)', re.IGNORECASE)
ANCHOR_RE = re.compile(r'<a\s+[^>]*href=["\']([^"\']+)["\'][^>]*>(.*?)</a>', re.IGNORECASE | re.DOTALL)

def _infer_reco_type(text: str) -> str:
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ ìœ í˜• ì¶”ì • (ìŒì•… ê´€ë ¨ í‚¤ì›Œë“œ ìˆìœ¼ë©´ music, ì•„ë‹ˆë©´ content)"""
    t = text.lower()
    music_kw = ["ìŒì•…", "ë…¸ë˜", "ê³¡", "ë®¤ì§", "playlist", "í”Œë ˆì´ë¦¬ìŠ¤íŠ¸", "song", "track"]
    return "music" if any(k in t for k in music_kw) else "content"

def _extract_links(raw: str) -> List[Tuple[str, str]]:
    """í…ìŠ¤íŠ¸ì—ì„œ ë§í¬ (href, label) ì¶”ì¶œ"""
    found: List[Tuple[str, str]] = []
    for m in ANCHOR_RE.finditer(raw):
        href, label = m.group(1).strip(), m.group(2).strip()
        if href and (href, label) not in found:
            found.append((href, label or href))
    for m in URL_RE.finditer(raw):
        url = m.group(1).strip()
        if not any(url == h for h, _ in found):
            found.append((url, url))
    return found

def _limit_links(ai_text: str) -> str:
    """ì¶”ì²œ ìœ í˜•ì— ë”°ë¼ ë§í¬ ê°œìˆ˜ë¥¼ ì œí•œ"""
    reco_type = _infer_reco_type(ai_text)
    links = _extract_links(ai_text)
    limit = 1 if reco_type == "music" else 2
    links = links[:limit]
    # ê¸°ì¡´ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ë§í¬ ì œê±° í›„, ì œí•œëœ ë§í¬ë§Œ ë‹¤ì‹œ ë¶™ì´ê¸°
    cleaned = ANCHOR_RE.sub("", ai_text)
    cleaned = URL_RE.sub("", cleaned).strip()
    if links:
        link_htmls = [f'<a href="{href}" target="_blank">ğŸ”— {label}</a>' for href, label in links]
        cleaned += "<br>" + " ".join(link_htmls)
    return cleaned

# ======================================================================================
# í”„ë¡œì•¡í‹°ë¸Œ ì¹´ë“œ ê´€ë ¨ í—¬í¼
# ======================================================================================
def _session_id_from_request() -> str:
    # ì„¸ì…˜ ì‹ë³„ì ìš°ì„ ìˆœìœ„: form > header > fallback
    return (
        request.form.get("session_id")
        or request.headers.get("X-SESSION-ID")
        or "default-session"
    )

def _topic_hint_from_text(text: str) -> Optional[str]:
    """ì•„ì£¼ ê°€ë²¼ìš´ í† í”½ íŒíŠ¸ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    t = (text or "").lower()
    if any(k in t for k in ["study", "ê³¼ì œ", "ê³µë¶€", "ë ˆí¬íŠ¸", "task", "ì½”ë”©", "ê°œë°œ", "debug"]):
        return "work/study"
    if any(k in t for k in ["ë¶ˆì•ˆ", "ì´ˆì¡°", "ìŠ¤íŠ¸ë ˆìŠ¤", "anxious", "stress"]):
        return "stress"
    if any(k in t for k in ["ìš´ë™", "ìŠ¤íŠ¸ë ˆì¹­", "ê±·ê¸°", "ì‚°ì±…"]):
        return "health"
    return None

def _build_action_button(label: str, action: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    """í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë²„íŠ¼ í´ë¦­ ì‹œ action/payload ì „ì†¡í•´ ì‹¤í–‰"""
    return {"label": label, "action": action, "payload": payload}

def _build_suggestion_card(s_types: List[SuggestionType], emotion: str, reason: str) -> Dict[str, Any]:
    """ì‹¤í–‰í˜• ë²„íŠ¼ 2~3ê°œ í¬í•¨í•œ ì œì•ˆ ì¹´ë“œ"""
    title = f"ì§€ê¸ˆ ë„ì›€ì´ ë  ìˆ˜ë„ ìˆì–´ìš” ({emotion})"
    desc  = f"ê·¼ê±°: {reason}"

    buttons: List[Dict[str, Any]] = []
    card_type = "info"
    url_main: Optional[str] = None
    alt_links: List[Dict[str, str]] = []

    for t in s_types:
        if t == "music":
            card_type = "music"
            url_main = "https://www.youtube.com/watch?v=jfKfPfyJRdk"
            buttons.append(_build_action_button("ë¡œíŒŒì´ ì¬ìƒ", "play_audio", {"url": url_main}))
        elif t == "breathing":
            buttons.append(_build_action_button("3ë¶„ í˜¸í¡ ê°€ì´ë“œ", "start_breathing", {"duration_sec": 180}))
            alt_links.append({"title": "í˜¸í¡ ê°€ì´ë“œ ì½ê¸°", "url": "https://www.healthline.com/health/box-breathing"})
        elif t == "timer":
            buttons.append(_build_action_button("5ë¶„ ìŠ¤íŠ¸ë ˆì¹­ íƒ€ì´ë¨¸", "start_timer", {"duration_sec": 300}))
        elif t == "memo":
            buttons.append(_build_action_button("ì§€ê¸ˆ ë©”ëª¨í•˜ê¸°", "open_memo", {"template": "ë°©ê¸ˆ ëŠë‚€ ê°ì •/ìƒê° í•œ ì¤„"}))
        elif t == "info":
            url_main = url_main or "https://www.healthline.com/health/mental-health/self-soothing"
            buttons.append(_build_action_button("ì§§ì€ ì½ì„ê±°ë¦¬", "open_link", {"url": url_main}))

    random.shuffle(buttons)
    buttons = buttons[: max(2, min(3, len(buttons)))]
    return {
        "type": "proactive_suggestion",
        "title": title,
        "desc": desc,
        "buttons": buttons,
        "emotion": emotion,
        "timestamp": int(time.time()),
        # ì¹´ë“œ ê°„ë‹¨ ìŠ¤í‚¤ë§ˆ(í”„ë¡ íŠ¸ í˜¸í™˜ìš©)
        "url": url_main,
        "alt": alt_links,
        "reason": reason,
        "card_type": card_type,
        "type_key": card_type
    }

# ======================================================================================
# ê³µí†µ I/O
# ======================================================================================
def get_openai_client(api_key: str):
    if not api_key:
        abort(401, description="OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    return AsyncOpenAI(api_key=api_key)

def upload_log_to_vercel_blob(blob_name: str, data: dict):
    if not VERCEL_TOKEN or not VERCEL_PROJ_ID:
        print("Vercel í™˜ê²½ë³€ìˆ˜(VERCEL_TOKEN, VERCEL_PROJECT_ID)ê°€ ì—†ì–´ ë¡œê·¸ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    try:
        b64_data = base64.b64encode(json.dumps(data, ensure_ascii=False).encode()).decode()
        resp = requests.post(
            "https://api.vercel.com/v2/blob",
            headers={"Authorization": f"Bearer {VERCEL_TOKEN}"},
            json={"projectId": VERCEL_PROJ_ID, "data": b64_data, "name": blob_name}
        )
        resp.raise_for_status()
        print(f"ë¡œê·¸ ì €ì¥ ì„±ê³µ: {blob_name}")
    except Exception as e:
        print(f"Vercel Blob ë¡œê·¸ ì—…ë¡œë“œ ì˜ˆì™¸: {e}")

# ======================================================================================
# ë©”ì¸ ì²˜ë¦¬(ë‹¨ë°œ ì™„ì„± ì‘ë‹µ) â€” ê¸°ì¡´ APIì™€ í˜¸í™˜
# ======================================================================================
async def process_chat(req):
    try:
        if 'audio' not in req.files:
            return jsonify(error="ì˜¤ë””ì˜¤ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."), 400
        api_key   = req.headers.get('X-API-KEY')
        character = req.form.get('character', 'kei')
        session_id = _session_id_from_request()
        client   = get_openai_client(api_key)

        # 1) Whisper STT
        audio_file = req.files['audio']
        stt_result = await client.audio.transcriptions.create(
            file=("audio.webm", audio_file.read()),
            model="whisper-1",
            response_format="text"
        )
        user_text = stt_result or ""

        # 2) ê°ì • ë¶„ì„ (JSON)
        emotion_resp = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": 'ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ë¶ˆêµì˜ ì¹ ì •(í¬,ë…¸,ì• ,ë‚™,ì• (ì‚¬ë‘),ì˜¤,ìš•)ì— ëŒ€í•´ '
                               'JSON í˜•ì‹({"percent": {...}, "top_emotion": "ê°ì •"})ìœ¼ë¡œ ë¶„ì„í•´ì¤˜.'
                },
                {"role": "user", "content": user_text}
            ],
            temperature=0.0,
            max_tokens=200,
            response_format={"type": "json_object"}
        )
        emotion_data    = json.loads(emotion_resp.choices[0].message.content)
        emotion_percent = emotion_data.get("percent", {})
        top_emotion     = emotion_data.get("top_emotion", "í¬")

        # 3) ë©”ì¸ ë‹µë³€ ìƒì„±
        system_prompt = CHARACTER_SYSTEM_PROMPTS[character]
        with history_lock:
            messages = [{"role": "system", "content": system_prompt}] + conversation_history[-HISTORY_MAX_LEN:]

        needs_web_search = top_emotion in ["ë…¸", "ì• ", "ì˜¤"]
        ai_text = ""
        audio_b64 = ""
        youtube_link = None

        # =====================[ ì›¹ ê²€ìƒ‰ ë¶„ê¸° ]=====================
        if needs_web_search:
            user_prompt = (
                f"{user_text}\n"
                f"(ì‚¬ìš©ìê°€ '{top_emotion}' ê°ì •ì„ ëŠë¼ê³  ìˆìŠµë‹ˆë‹¤. ë”°ëœ»í•œ ìœ„ë¡œì˜ ë§ê³¼ í•¨ê»˜ ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•´ ê´€ë ¨ëœ ìœ„ë¡œê°€ ë˜ëŠ” ìœ íŠœë¸Œ ìŒì•… URLì„ ì°¾ì•„ ì œì•ˆí•´ì£¼ì„¸ìš”.)\n"
                "ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¡œ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n"
                "1. ê³µê°ì˜ í•œë§ˆë””\n"
                "2. ìƒí™©ì— ì–´ìš¸ë¦¬ëŠ” ì œì•ˆ(ì´ëŸ´ ë•ŒëŠ” ~ ì–´ë–¤ê°€ìš”?)\n"
                "3. ì œì•ˆì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…"
            )
            messages.append({"role": "user", "content": user_prompt})

            search_response = await client.chat.completions.create(
                model="gpt-4o-mini-search-preview",
                messages=messages,
            )
            result = search_response.choices[0]
            content = result.message.content
            annotations = getattr(result.message, 'annotations', None) or []

            ai_text = content
            link_list: List[str] = []
            for ann in annotations:
                if getattr(ann, "type", None) == "url_citation":
                    url = ann.url_citation.url
                    start = ann.url_citation.start_index
                    end = ann.url_citation.end_index
                    link_text = content[start:end]
                    a_tag = f'<a href="{url}" target="_blank">{link_text}</a>'
                    ai_text = ai_text[:start] + a_tag + ai_text[end:]
                    link_list.append(url)

            ai_text = markdown_to_html_links(ai_text)
            # (ì˜µì…˜) ë§í¬ ê³¼ë‹¤ì‹œ ì œí•œ
            # ai_text = _limit_links(ai_text)

            if link_list:
                youtube_link = link_list[0]
            else:
                youtube_link = extract_first_markdown_url(content)
                if not youtube_link:
                    candidates = EMOTION_LINKS.get(top_emotion, [])
                    if candidates:
                        _, youtube_link = random.choice(candidates)
                    else:
                        youtube_link = None
            if youtube_link and youtube_link not in ai_text:
                ai_text += f'<br><a href="{youtube_link}" target="_blank">â–¶ï¸ ì¶”ì²œ ìŒì•… ë°”ë¡œ ë“£ê¸°</a>'

            # TTS í…ìŠ¤íŠ¸(ë§í¬ ì œê±°/ì´ëª¨ì§€ ì œê±°)
            tts_text = remove_empty_parentheses(content)
            tts_text = remove_emojis(tts_text)
            offset = 0
            for ann in annotations:
                if getattr(ann, "type", None) == "url_citation":
                    start = ann.url_citation.start_index - offset
                    end = ann.url_citation.end_index - offset
                    tts_text = tts_text[:start] + tts_text[end:]
                    offset += (end - start)
            tts_text = tts_text.strip()

            audio_response = await client.audio.speech.create(
                model="gpt-4o-mini-tts",
                voice=CHARACTER_VOICE[character],
                input=tts_text
            )
            audio_b64 = base64.b64encode(audio_response.content).decode()

        # =====================[ ì¼ë°˜ ë¶„ê¸° ]=====================
        else:
            if top_emotion in ["í¬", "ë‚™", "ì• (ì‚¬ë‘)"]:
                user_prompt = (
                    f"{user_text}\n"
                    f"(ì‚¬ìš©ìê°€ '{top_emotion}' ê°ì •ì„ ëŠë¼ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ìƒí™©ì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•˜ë©° ê³µê°í•´ì£¼ì„¸ìš”.)\n"
                )
            elif top_emotion == "ìš•":
                user_prompt = (
                    f"{user_text}\n"
                    f"(ì‚¬ìš©ìê°€ '{top_emotion}' ê°ì •ì„ ëŠë¼ê³  ìˆìŠµë‹ˆë‹¤. ì‘ì›ì˜ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”.)\n"
                )
            else:
                user_prompt = (
                    f"{user_text}\n"
                    "ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¡œ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n"
                    "1. ê³µê°ì˜ í•œë§ˆë””\n"
                    "2. ìƒí™©ì— ì–´ìš¸ë¦¬ëŠ” ì œì•ˆ(ì´ëŸ´ ë•ŒëŠ” ~ ì–´ë–¤ê°€ìš”?)\n"
                    "3. ì œì•ˆì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…"
                )
            messages.append({"role": "user", "content": user_prompt})

            response = await client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.7,
                max_tokens=512,
            )
            ai_text = response.choices[0].message.content or ""
            ai_text = remove_emojis(ai_text)
            if not ai_text:
                ai_text = "ì•„ì§ ë‹µë³€ì„ ì¤€ë¹„í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"

            # (ì˜µì…˜) ë§í¬ í›„ì²˜ë¦¬
            # ai_text = markdown_to_html_links(ai_text)
            # ai_text = _limit_links(ai_text)

            tts_text = re.sub(r'ë§í¬:.*', '', ai_text).strip()
            tts_text = remove_emojis(tts_text)

            audio_response = await client.audio.speech.create(
                model="gpt-4o-mini-tts",
                voice=CHARACTER_VOICE[character],
                input=tts_text
            )
            audio_b64 = base64.b64encode(audio_response.content).decode()
            youtube_link = None

        # 4) ëŒ€í™” ê¸°ë¡ ê°±ì‹ 
        now_kst_iso = datetime.datetime.now(datetime.timezone.utc).isoformat() + "Z"
        with history_lock:
            conversation_history.append({"role": "user", "content": user_text, "ts": now_kst_iso})
            conversation_history.append({"role": "assistant", "content": ai_text, "ts": now_kst_iso})
            if len(conversation_history) > HISTORY_MAX_LEN:
                conversation_history[:] = conversation_history[-HISTORY_MAX_LEN:]

        # ---------------- í”„ë¡œì•¡í‹°ë¸Œ íŒë‹¨/ì¹´ë“œ ìƒì„± ----------------
        last_ts = _last_user_utter_ts.get(session_id, 0.0)
        now_ts  = time.time()
        silence_sec = now_ts - last_ts if last_ts > 0 else 0.0
        _last_user_utter_ts[session_id] = now_ts

        topic_hint = _topic_hint_from_text(user_text)
        suggest_res = _policy.should_suggest(
            sid=session_id,
            emotion=top_emotion,
            last_utter_silence_sec=silence_sec,
            topic=topic_hint
        )

        proactive_card: Optional[Dict[str, Any]] = None
        if suggest_res.get("ok"):
            s_types = _policy.choose_suggestion_types(session_id)
            reason  = f"ê°ì •={top_emotion}, ì¹¨ë¬µ={int(silence_sec)}s, topic={topic_hint or '-'}"
            proactive_card = _build_suggestion_card(s_types, top_emotion, reason)
            _policy.stamp_suggested(session_id, reason)

        # ë¡œê·¸ ì—…ë¡œë“œ (ë¹„ë™ê¸°)
        log_data = {
            "timestamp": now_kst_iso,
            "session_id": session_id,
            "character": character,
            "user_text": user_text,
            "emotion_percent": emotion_percent,
            "top_emotion": top_emotion,
            "ai_text": ai_text,
            "proactive_card": proactive_card or None
        }
        now = datetime.datetime.now(datetime.timezone.utc)
        blob_name = f"logs/{now.strftime('%Y-%m-%dT%H-%M-%SZ')}_{character}.json"
        asyncio.create_task(asyncio.to_thread(upload_log_to_vercel_blob, blob_name, log_data))

        # ì‘ë‹µ
        return jsonify({
            "user_text": user_text,
            "ai_text": remove_empty_parentheses(ai_text),
            "audio": audio_b64,
            "emotion_percent": emotion_percent,
            "top_emotion": top_emotion,
            "link": youtube_link,
            "proactive_card": proactive_card
        })

    except Exception as e:
        import traceback; traceback.print_exc()
        return jsonify({"error": f"Failed to process request: {e}"}), 500

# ======================================================================================
# ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬(SSE ìŠ¤íƒ€ì¼) â€” /scripts/chat_stream ì—ì„œ ì‚¬ìš©
# ======================================================================================
async def stream_chat(req):
    """
    í† í° ë‹¨ìœ„ë¡œ ì „ì†¡ í›„, ë§ˆì§€ë§‰ì— ìµœì¢… íŒ¨í‚·(ai_text/html, audio_b64, emotion, proactive_card) ì†¡ì‹ 
    Front: fetch('/scripts/chat_stream', ...) + ReadableStream íŒŒì‹±(chat.js ì°¸ê³ )
    """
    if 'audio' not in req.files:
        return jsonify(error="ì˜¤ë””ì˜¤ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."), 400

    api_key   = req.headers.get('X-API-KEY')
    character = req.form.get('character', 'kei')
    session_id = _session_id_from_request()
    client    = get_openai_client(api_key)

    # 1) STT
    audio_file = req.files['audio']
    stt_result = await client.audio.transcriptions.create(
        file=("audio.webm", audio_file.read()),
        model="whisper-1",
        response_format="text"
    )
    user_text = stt_result or ""

    # 2) ê°ì • ë¶„ì„
    emotion_resp = await client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system",
             "content": 'ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ë¶ˆêµì˜ ì¹ ì •(í¬,ë…¸,ì• ,ë‚™,ì• (ì‚¬ë‘),ì˜¤,ìš•)ì— ëŒ€í•´ '
                        'JSON í˜•ì‹({"percent": {...}, "top_emotion": "ê°ì •"})ìœ¼ë¡œ ë¶„ì„í•´ì¤˜.'},
            {"role": "user", "content": user_text}
        ],
        temperature=0.0,
        max_tokens=200,
        response_format={"type": "json_object"}
    )
    emotion_data    = json.loads(emotion_resp.choices[0].message.content)
    emotion_percent = emotion_data.get("percent", {})
    top_emotion     = emotion_data.get("top_emotion", "í¬")

    # 3) ìŠ¤íŠ¸ë¦¬ë°ìš© ë©”ì‹œì§€ êµ¬ì„±
    system_prompt = CHARACTER_SYSTEM_PROMPTS[character]
    with history_lock:
        messages = [{"role": "system", "content": system_prompt}] + conversation_history[-HISTORY_MAX_LEN:]
        messages.append({"role": "user", "content": user_text})

    needs_web_search = top_emotion in ["ë…¸", "ì• ", "ì˜¤"]
    if needs_web_search:
        messages[-1] = {"role": "user", "content":
            f"{user_text}\n(ë”°ëœ»í•œ ìœ„ë¡œ + ê´€ë ¨ ìœ íŠœë¸Œ ìŒì•… URL ì œì•ˆ)\n2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ ë‹µë³€"}
        model_name = "gpt-4o-mini-search-preview"
    else:
        model_name = "gpt-4o"

    async def event_stream():
        # LLM ìŠ¤íŠ¸ë¦¼
        stream = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=0.7,
            max_tokens=512,
            stream=True
        )
        full_text: List[str] = []

        async for chunk in stream:
            delta = chunk.choices[0].delta.get("content")
            if delta:
                full_text.append(delta)
                yield f"event: token\ndata: {json.dumps({'token': delta}, ensure_ascii=False)}\n\n"

        final_text = "".join(full_text).strip() or "ì•„ì§ ë‹µë³€ì„ ì¤€ë¹„í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
        final_text_noemoji = remove_emojis(final_text)

        # --- í›„ì²˜ë¦¬ ë™ì‹œ ì‹¤í–‰: ë§í¬/ì¹´ë“œ/ë¡œê·¸/TTS ---
        async def build_final_payload():
            # ë§í¬ HTMLí™”
            ai_text_html = markdown_to_html_links(final_text_noemoji)
            # ai_text_html = _limit_links(ai_text_html)  # (ì˜µì…˜)

            # í”„ë¡œì•¡í‹°ë¸Œ ì¹´ë“œ
            proactive_card = None
            try:
                topic_hint = _topic_hint_from_text(user_text)
                last_ts = _last_user_utter_ts.get(session_id, 0.0)
                now_ts  = time.time()
                silence_sec = now_ts - last_ts if last_ts > 0 else 0.0
                _last_user_utter_ts[session_id] = now_ts

                suggest_res = _policy.should_suggest(session_id, top_emotion, silence_sec, topic_hint)
                if suggest_res.get("ok"):
                    s_types = _policy.choose_suggestion_types(session_id)
                    reason  = f"ê°ì •={top_emotion}, ì¹¨ë¬µ={int(silence_sec)}s, topic={topic_hint or '-'}"
                    proactive_card = _build_suggestion_card(s_types, top_emotion, reason)
                    _policy.stamp_suggested(session_id, reason)
            except Exception:
                proactive_card = None

            # ë¡œê·¸ ì—…ë¡œë“œ
            now_kst_iso = datetime.datetime.now(datetime.timezone.utc).isoformat() + "Z"
            log_data = {
                "timestamp": now_kst_iso,
                "session_id": session_id,
                "character": character,
                "user_text": user_text,
                "emotion_percent": emotion_percent,
                "top_emotion": top_emotion,
                "ai_text": ai_text_html,
                "proactive_card": proactive_card
            }
            now = datetime.datetime.now(datetime.timezone.utc)
            blob_name = f"logs/{now.strftime('%Y-%m-%dT%H-%M-%SZ')}_{character}.json"
            asyncio.create_task(asyncio.to_thread(upload_log_to_vercel_blob, blob_name, log_data))

            # TTS
            audio_b64 = ""
            try:
                tts_text = re.sub(r'ë§í¬:.*', '', final_text_noemoji).strip()
                audio_response = await client.audio.speech.create(
                    model="gpt-4o-mini-tts",
                    voice=CHARACTER_VOICE[character],
                    input=tts_text
                )
                audio_b64 = base64.b64encode(audio_response.content).decode()
            except Exception:
                pass

            return {
                "ai_text": ai_text_html,
                "audio": audio_b64,
                "emotion_percent": emotion_percent,
                "top_emotion": top_emotion,
                "proactive_card": proactive_card
            }

        payload = await build_final_payload()
        yield f"event: final\ndata: {json.dumps(payload, ensure_ascii=False)}\n\n"

    return Response(stream_with_context(event_stream()), mimetype="text/event-stream")

# ======================================================================================
# í”„ë¡œì•¡í‹°ë¸Œ í”¼ë“œë°± ìˆ˜ì§‘ â€” /proactive/feedback
# ======================================================================================
def proactive_feedback():
    """
    JSON: {"session_id": "...", "suggestion_type": "music|breathing|timer|memo|info", "accepted": true/false}
    """
    try:
        data = request.get_json(force=True, silent=True) or {}
        session_id = data.get("session_id") or _session_id_from_request()
        suggestion_type = data.get("suggestion_type", "info")
        accepted = bool(data.get("accepted", False))

        stype: SuggestionType = suggestion_type if suggestion_type in ["music","breathing","timer","memo","info"] else "info"
        _policy.feedback(session_id, stype, accepted)
        st = _policy.state_of(session_id)
        return jsonify({"ok": True, "weights": st.pref_weights, "accepts": st.accepts, "rejects": st.rejects})
    except Exception as e:
        import traceback; traceback.print_exc()
        return jsonify({"ok": False, "error": str(e)}), 500
